---
title: "Hu.ad699.Assignment 2"
output: html_notebook
---


```{r}
library(tidyverse)
library(caret)
library(forecast)
```

```{r}
#1. set the working directory and Read CSV files
setwd("C:/Users/Trista Hu/OneDrive/BU ABA Study Summer/AD699 Data Mining Summer/Assignment 2")
day <- read.csv("day.csv")
head(day)
```

```{r}
#2. check the structure of data
str(day)
```
Based on the dataset description and the result above, 
Numeric variables are: instant, dteday, temp, atemp, hum, windspeed, casual, registered, cnt
Categorical variables are: season, yr, mnth, holiday, weekday, workingday, weathersit

```{r}
#3. Creating a data partition with 60% training set and 40% validation set
set.seed(120)  
train_indices <- sample(c(1:nrow(day)),nrow(day)*0.6)
training_set <- day[train_indices, ]
validation_set <- day[-train_indices, ]
dim(training_set) #test on the partition
dim(validation_set)
```
3a.
For predictive models, it's important to gauge how effectively they work with unobserved data. By splitting the data, modelers can train the model on the training set and assess its effectiveness on a different subset (the validation or testing set). So, data partitioning helps in evaluating the model's performance on unseen data.
If modelers analyze the entire dataset before splitting, there's a risk of overfitting, meaning the model does not generalize beyond the training set, hence it is likely to perform poorly on new data. Partitioning allows modelers to train the model on one subset and test it on another, simulating real-world scenarios. So modelers can spot overfitting and create more reliable models by utilizing a different validation set.

```{r}
#4. Scatterplot with best-fit line using training set data
library(ggplot2)
plot1<-ggplot(training_set, aes(x = temp, y = cnt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Temperature", y = "Total Bike Rentals")
plot1
```

The best-fit line has a positive slope, suggesting that bike rentals increase with higher temperatures.
The positive correlation makes sense to me as it aligns with common observations and human behavior. When the weather is warmer, people tend to engage more in outdoor activities. Warmer temperatures create a more pleasant environment for biking, making it a joyful experience for riders.

```{r}
#5. Correlation between cnt and temp
correlation <- cor(training_set$cnt, training_set$temp)

#Test significance 
cor_test <- cor.test(training_set$cnt, training_set$temp)

correlation  # Correlation value
cor_test$p.value  # p-value for the correlation test
```
The correlation value of 0.6287367 indicates a moderately strong positive correlation between cnt and temp, and the very small p-value suggests that this correlation is highly significant and unlikely to be due to random chance. 

```{r}
#6. Simple linear regression model
slr_model <- lm(cnt ~ temp, data = training_set)

summary(slr_model)
```

```{r}
#7. Get minimum and maximum residual values
slr_model$residuals

min_residual <- min(slr_model$residuals)
max_residual <- max(slr_model$residuals)

min_residual  # Minimum residual value
max_residual  # Maximum residual value
```
Minimum residual value is -3483.698.
Maximum residual value is 3553.5. 

```{r}
#7a. Find the observation with the highest residual
highest_residual_index <- which.max(slr_model$residuals)
highest_residual_index
# Actual count of bicycle rentals that day
actual_cnt <- training_set$cnt[highest_residual_index]
actual_cnt
# Predicted count from the model
predicted_cnt <- predict(slr_model, newdata = training_set)[highest_residual_index]
predicted_cnt
# Residual calculation: Residual = Actual count - Predicted count
residual <- actual_cnt - predicted_cnt
residual
```
Actual count of bicycle rentals on that day is 8714. 
Model's predicted count is 5160.5.
Residual value is 3553.5.

```{r}
#7b. Find the observation with the lowest residual
lowest_residual_index <- which.min(slr_model$residuals)
actual_cnt_lowest <- training_set$cnt[lowest_residual_index]
actual_cnt_lowest
predicted_cnt_lowest <- predict(slr_model, newdata = training_set)[lowest_residual_index]
predicted_cnt_lowest
residual_lowest <- actual_cnt_lowest - predicted_cnt_lowest
residual_lowest
```
Actual count of bicycle rentals on that day is 1842.
Model's predicted count is 5325.698.
Residual value is -3483.698.

7c. 
"Temp" represents normalized temperature in Celsius; it is just one of the possible predictor variables, and it's even not a very strong predictor for predicting the total rental bikes (seeing the correlation value). There are other variables, such as whether it's a holiday, good weather conditions, or strong winds, which could potentially be important predictors (based on common practice). However, in this Simple Linear Regression model, the correlation between these variables and bike rentals has not been tested, thus the impact of these potential predictors has not been included yet. As a result, there might be significant residuals in the model.

```{r}
#9. Regression equation generated by the model
regression_eq <- paste("cnt =", round(slr_model$coefficients[1], 2), "+", round(slr_model$coefficients[2], 2), "* temp")
regression_eq

```
Regression equation: cnt = 1231.81 + 6458.12 * temp

```{r}
#9. Selected temp_hypothetical = 0.255)
temp_hypothetical <- 0.255
predicted_cnt_hypothetical <- predict(slr_model, newdata = data.frame(temp = temp_hypothetical))
predicted_cnt_hypothetical
```
The regression equation, cnt = 1231.81 + 6458.12 * temp, represents the relationship between the total count of bike rentals (cnt) and the normalized temperature (temp).

By substituting temp = 0.255 into the regression equation, we can calculate the predicted outcome for the total count of bike rentals: cnt = 1231.81 + 6458.12 * 0.255

Calculating the result and rounding to an appropriate decimal place: cnt â‰ˆ 2878.633 
So, using temp = 0.255 as the hypothetical normalized temperature, the predicted outcome for the total count of bike rentals is approximately 2878.633.

```{r}
#10.
library(forecast)
# Predictions on training set
predictions_train <- predict(slr_model, newdata = training_set)
# Predictions on validation set
predictions_validation <- predict(slr_model, newdata = validation_set)

# RMSE and MAE on training set
rmse_train <- accuracy(predictions_train, training_set$cnt)[2]
mae_train <- accuracy(predictions_train, training_set$cnt)[3]
rmse_train
mae_train
# RMSE and MAE on validation set
rmse_validation <- accuracy(predictions_validation, validation_set$cnt)[2]
mae_validation <- accuracy(predictions_validation, validation_set$cnt)[3]
rmse_validation
mae_validation
```
RMSE and MAE on training set: 1500.694, 1243.584
RMSE and MAE on validation set: 1522.5, 1247.751

Comparing the RMSE and MAE between the training set and the validation set helps in assessing the model's generalization ability. A model with a significantly lower RMSE and MAE on the training set compared to the validation set might be overfitting, which referring to when the model performs very well on the training data but fails to generalize to new data, thus poor performance on the validation set.

On the other hand, if the RMSE and MAE are similar or only slightly higher on the validation set compared to the training set, it suggests that the model has learned the underlying patterns and relationships without perfectly mapping the training data. Such a model is more likely to perform well on new, unseen data.

Based on the obtained RMSE and MAE value, the performance of the model on training set and validation set is close and thus low risk of overfitting.

```{r}
#11. Comparing RMSE with standard deviation of bicycle rentals in the training set
std_dev_cnt <- sd(training_set$cnt)
std_dev_cnt

rmse_train / std_dev_cnt
```
standard deviation of bicycle rentals in the training set: 1932.066
RMSE vs. Standard Deviation: 0.77673

When the RMSE (with "0.77673") is much lower than the standard deviation of the bicycle rentals in the training set, it indicates that a significant portion of the variability in the data is being captured by the model. As a result, the model's predictions are relatively close to the true values, and the residuals are smaller than the fluctuations in rental counts that occur naturally.
On the other hand, if the RMSE is nearer to or more than the standard deviation, it might be a hint that the model isn't working well because its forecasts aren't much better than those made by merely utilizing the mean value as a forecast.

```{r}
#Multiple Linear Regression:

#1a.
day$result <- day$registered + day$casual - day$cnt
result_value_count <- table(day$result)
print(result_value_count)
```
All the result of "registered" + "casual" - "cnt" are 0. This proved "cnt" is the sum of "registered" + "casual". So "registered" and "casual" need to be removed 
The reason for excluding these variables is to prevent introducing multicollinearity into the model. Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated with each other. When predictor variables are highly correlated, it becomes difficult for the model to distinguish their individual effects on the outcome variable. As a result, the model might assign inconsistent or unstable coefficient estimates to these variables, leading to misleading interpretations of their impact on the outcome.
```{r}
#removing registered and casual
training_set <- subset(training_set, select = -c(registered, casual))
validation_set <- subset(validation_set, select = -c(registered, casual))

```

```{r}
#1b. Remove index and dteday variables from training and validation sets
training_set <- subset(training_set, select = -c(instant, dteday))
validation_set <- subset(validation_set, select = -c(instant, dteday))

```

Models are built based on patterns/information from trends and differences in data. There is no variability or pattern in a categorical variable if it has a different value for every record. Since each value is distinct and has no ability to distinguish one record from another, categorical variables with unique values give no information benefit, rendering it useless for making predictions. 

```{r}
#1c.1d. Remove mnth and yr variables
training_set <- subset(training_set, select = -c(mnth, yr))
validation_set <- subset(validation_set, select = -c(mnth, yr))
```


```{r}
#2. Convert categorical variables to factors in training and validation sets
str(training_set) #"season", "holiday", "weekday", "workingday", "weathersit" are not factors so far
categorical_vars <- c("season", "holiday", "weekday", "workingday", "weathersit")
training_set[categorical_vars] <- lapply(training_set[categorical_vars], as.factor)
validation_set[categorical_vars] <- lapply(validation_set[categorical_vars], as.factor)
str(training_set)
str(validation_set)
```

```{r}
#3. 
# Create correlation matrix for numerical variables in the training set
correlation_matrix <- cor(training_set[, sapply(training_set, is.numeric)])
correlation_matrix #temp&atemp has a correlations > 0.80

training_set <- subset(training_set, select = -temp)
validation_set <- subset(validation_set, select = -temp)
str(training_set)
str(validation_set)
```
Temp & atemp have a correlation 0.9889026 > 0.80, which suggests a risk of multicollinearity. Therefore, one of these variables should be removed from both the training and validation sets. I chose to remove "temp" as is commonly done, considering that what is more directly related to determining whether to take a bike is how people feel about the temperature rather than the actual temperature number. For example, in Boston during winter, the temperature might seem fine, but the feeling temperature could be much lower, dissuading people from taking a ride. Therefore, data from the perceived temperature could have a bigger impact on the prediction result.

4.
Dummy variables are a form of category variable used in statistical modeling and machine learning. The purpose is to use them in regression and other mathematical models that need numerical input. They are designed to represent categorical data with two or more categories as binary values (0 or 1). With the use of dummy variables, categorical data can be incorporated into the model's computations in order to capture the impacts of category variables that lack a natural numerical representation.

```{r}
#5. Perform backward elimination using step() function
mlr_model <- lm(cnt ~ ., data = training_set)
mlr_model <- step(mlr_model)

#5a.
summary(mlr_model)

```


```{r}
#6a. calculate the SST
mean_cnt <- mean(training_set$cnt)
SST <- sum((training_set$cnt - mean_cnt)^2)
SST
```
SST is 1631268453.

```{r}
#6b. calculate SSR
fitted_values <- fitted(mlr_model)
SSR <- sum((fitted_values - mean_cnt)^2)
SSR
```
SSR is 909729224.

```{r}
#6c. Calculate SSR / SST ratio
SSR_over_SST <- SSR / SST
SSR_over_SST
```
SSR / SST ratio = 0.5576821

We can find the ratio in the summary with the R-squared value, which is essentially SSR divided by SST. 

```{r}
#7. 
summary(mlr_model)
install.packages(visualize)
library(visualize)
df_model <- df.residual(mlr_model)
df_model
# selected "holiday1", t-value is -2.619
plot2<-visualize.t(stat=c(-2.619, 2.619), df = 428, section="bounded")
plot2

```
7. 99.1% of the curve is shaded. 
The p-value for the predictor is equal to 1 minus the shaded area of 0.991, which is 0.0091 as we can see from the summary results.

8.
Based on the model summary, the F-statistic: 59.96.

The F-statistic in multiple linear regression tests the overall significance of the regression model and provides a measure of how well the model fits the data by comparing the explained variance to the unexplained variance. A significant F-statistic indicates that the model is useful in explaining the variability in the dependent variable. The better the fit, the higher the F-score will be.


```{r}
#demonstrate where the F-Statistic comes from
k <- 9
n <- 438

SSR <- sum((mlr_model$fitted.values - mean(training_set$cnt))^2)
SSE <- sum(mlr_model$residuals^2)
numerator <- SSR / k
denominator <- SSE / (438-k-1)
numerator/denominator
```

```{r}
#9.
fiction_day <- data.frame(season = as.factor(3),
  holiday = as.factor(0),
  weathersit = as.factor(2),
  atemp = 0.2332,
  hum = 0.4369,
  windspeed = 0.1602)

# Now, predict using the mlr_model
fiction_pred<-predict(mlr_model,fiction_day)
fiction_pred
```
Based on the fictional data inputs for predictor variables, the model predict this dayâ€™s bicycle rental total 3612.63. 

```{r}
#10.
# Predict on the training set using the trained mlr_model
train_pred <- predict(mlr_model, newdata = training_set)

# Predict on the validation set using the trained mlr_model
validation_pred <- predict(mlr_model, newdata = validation_set)

# Calculate accuracy measures for both sets
train_accuracy <- accuracy(train_pred, training_set$cnt)
validation_accuracy <- accuracy(validation_pred, validation_set$cnt)

# Print the accuracy results
cat("Training set accuracy:\n")
print(train_accuracy)

cat("\nValidation set accuracy:\n")
print(validation_accuracy)
```
While the RMSE and MAE are slightly higher on the validation set compared to the training set, they are still reasonable.

The difference between the training set and validation set performance is relatively small, suggesting that the model is not overfitting the training data. Overfitting occurs when a model performs very well on the training data but poorly on unseen data. In this case, the risk of overfitting seems to be low.

In terms of accuracy, the multiple linear regression model tends to perform better than the simple linear regression model. This is evident from the fact that the R-squared value in the MLR model (0.5577) is higher than the R-squared value in the SLR model (0.3953). The MLR model considers additional predictor variables, allowing it to capture more complex relationships between the predictors and the dependent variable, resulting in better predictive performance.
